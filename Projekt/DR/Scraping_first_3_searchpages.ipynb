{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40edec5",
   "metadata": {},
   "source": [
    "# Google DR Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69b9bd",
   "metadata": {},
   "source": [
    "## Questions\n",
    "* It seems that with Selenium, we do not provide the names. We only do when using the Request packages...:headers={\"Name\" : \"Simon Ullrich - summer course project\" , \"email\": \"simon.ullrich@sodas.ku.dk\"}\n",
    "\n",
    "## Comments\n",
    "* Looping until last page implemented\n",
    "    * Error term included to do while loop\n",
    "* Time.sleep included for less suspicious behaviour\n",
    "* CSS selector changed, because if last page is reached, the old CSS selecter might just have turned around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870ed07",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38e2a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import csv\n",
    "\n",
    "# Import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d37bfa",
   "metadata": {},
   "source": [
    "## Generate search terms\n",
    "\n",
    "Search terms follow the following format: We search for the month and year in the format DR articles inclue a timestap. Manual Google searches proved to provide relevant research results mostly limited to the month provided. In this way we create a list of links to DR articles. All articles are located on the site https://www.dr.dk/nyheder or a subsite. This can be included in the google search. An example of a search is: jan. 2012\" AND \"sygeplejersker*\" site:https://www.dr.dk/nyheder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb2ff806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate empty list with search terms\n",
    "search_terms = []\n",
    "# Generate combinations of year-month and term search combinations\n",
    "for year in range(2012, 2023):\n",
    "    months = ['jan.', 'feb.', 'mar.', 'apr.', 'maj', 'jun.', 'jul.', 'aug.', 'sep.', 'okt.', 'nov.', 'dec.']\n",
    "    for month in months:\n",
    "        term = f'\"{month} {year}\" AND \"sygeplejersker*\" site:https://www.dr.dk/nyheder/ \\n'\n",
    "        search_terms.append(term)\n",
    "# Create final list of search terms, future month-year combinations deleted\n",
    "search_terms = search_terms[:-4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72117573",
   "metadata": {},
   "source": [
    "We create a total of 124 search terms, for all month-year combinations between January 2012 and August 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b08e8b",
   "metadata": {},
   "source": [
    "## Scraping Google search to retrieve list of links to DR articles\n",
    "With the following code we scrape Google searches to retrieve a list of links to DR articles, given the DR website does not provide a useful search function. We use Selenium to go execute a Google search and retrieve DR article links. We execute the search, save the HTML for the first results page and then go to further pages of the search results to retrieve more search resutls. Google intervenes when scaping search results too fast. We therefore integrate a break when moving between pages. The break time takes random values between 0.25 and 3.5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54ab1a",
   "metadata": {},
   "source": [
    "### Scraping the first three search pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3dec450f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21476/469254787.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhtml_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfinished_searches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_terms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdriverService\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mService\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\jgb569\\OneDrive - University of Copenhagen\\Documents\\Webscraping\\chromedriver.exe\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriverService\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "html_list = []\n",
    "finished_searches = 0\n",
    "for i in tqdm.tqdm(search_terms[0:124]):\n",
    "    driverService = Service(r\"C:\\Users\\jgb569\\OneDrive - University of Copenhagen\\Documents\\Webscraping\\chromedriver.exe\") \n",
    "    driver = webdriver.Chrome(service = driverService)\n",
    "    # Go to google\n",
    "    driver.get('https:google.com')\n",
    "    # Discard cookie message, reject cookies\n",
    "    cookie = driver.find_element(By.ID, \"W0wltc\")\n",
    "    cookie.click()\n",
    "    # Search for DR news articles\n",
    "    gsearch = driver.find_element(By.CSS_SELECTOR, \"input[title='SÃ¸g']\")\n",
    "    gsearch.send_keys(i)\n",
    "    # Get HTML for first search result page\n",
    "    html = driver.page_source\n",
    "    html_list.append(html)\n",
    "    # Go to next result page\n",
    "    next_page = driver.find_element(By.CSS_SELECTOR, \".NVbCr+ span\") #CSS selector only last not previous page\n",
    "    next_page.click()\n",
    "    # Define an error used when reaching last search page:\n",
    "        # When error = 0, there is another resut page.\n",
    "        # When error = 1, there is no further page on Google, loop stops.\n",
    "    error = 0\n",
    "    page = 1\n",
    "    for page in range(0,1): #Iterates over 3 Pages in total\n",
    "        try:\n",
    "            html2 = driver.page_source\n",
    "            html_list.append(html2)\n",
    "            # Google detects suspicious behavior and asks to solve some puzzle after 7 iterations. Trying random sleep time and scrolling down to element.\n",
    "            time.sleep(np.random.uniform(10, 15))\n",
    "            # Go to next result page\n",
    "            next_page = driver.find_element(By.CSS_SELECTOR, \"#pnnext .NVbCr+ span\") #CSS selector only last not previous page\n",
    "            next_page.click()\n",
    "            page += 1\n",
    "        except:\n",
    "            error += 1\n",
    "    finished_searches += 1\n",
    "    time.sleep(np.random.uniform(5,10))\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5184b7",
   "metadata": {},
   "source": [
    "### Create link list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3be44fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing two empty lists\n",
    "link_list = []\n",
    "link_list_clean = []\n",
    "# Iterating over the results from scraping\n",
    "for l in html_list:\n",
    "    soup = BeautifulSoup(l, 'lxml')\n",
    "    try: \n",
    "        links = soup.find('div', class_ = 'v7W49e').find_all('a', href=True)\n",
    "    except:\n",
    "        pass\n",
    "    # Generate list with all links\n",
    "    for i in links:\n",
    "        temp = i['href']\n",
    "        link_list.append(temp)\n",
    "    # Getting rid of noise, links not pointing to DR but Google infrastructure\n",
    "    for link in link_list:\n",
    "        if \"webcache.googleusercontent\" not in link:\n",
    "            link_list_clean.append(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcd3e8",
   "metadata": {},
   "source": [
    "#### From list to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff743994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pd.DataFrame from link_list_clean\n",
    "dr_links = pd.DataFrame({'links':link_list_clean})\n",
    "# For some reason, there are many duplicates included, drop them\n",
    "dr_links = dr_links.drop_duplicates(subset = 'links')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8e26e9",
   "metadata": {},
   "source": [
    "#### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87437d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links.to_csv(\"dr_links.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
