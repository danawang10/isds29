{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f18b47-2b01-4e26-bc78-9cfbf44f8bc4",
   "metadata": {},
   "source": [
    "# Analysis of usage of the term \"sygeplejersker\" in DR, TV2 and the danish parliament"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd9881-a42a-421c-9a7d-843f59faa8a7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30278d2-007e-4853-af75-195303533cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165aac07-dbd5-4342-a684-52e424f60bd3",
   "metadata": {},
   "source": [
    "## Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55f149-b9a4-4e40-97d2-c027c053b8bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf08614-0d45-4cf2-b599-78f0e4da3f2c",
   "metadata": {},
   "source": [
    "> NOTE: The following code must be run on a computer with danish language as google behaves differently based on the computers current language settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9472f-f8af-4213-b601-9d23ec45c8e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1:  Generate search terms\n",
    "\n",
    "Search terms follow the following format: We search for the month and year in the format DR articles inclue a timestap. Manual Google searches proved to provide relevant research results mostly limited to the month provided. In this way we create a list of links to DR articles. All articles are located on the site https://www.dr.dk/nyheder or a subsite. This can be included in the google search. An example of a search is: jan. 2012\" AND \"sygeplejersker*\" site:https://www.dr.dk/nyheder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbddb6dd-1a86-4b77-bbfd-7856ae2a4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate empty list with search terms\n",
    "search_terms = []\n",
    "# Generate combinations of year-month and term search combinations\n",
    "for year in range(2012, 2023):\n",
    "    months = ['jan.', 'feb.', 'mar.', 'apr.', 'maj', 'jun.', 'jul.', 'aug.', 'sep.', 'okt.', 'nov.', 'dec.']\n",
    "    for month in months:\n",
    "        term = f'\"{month} {year}\" AND \"sygeplejersker*\" site:https://www.dr.dk/nyheder/ \\n'\n",
    "        search_terms.append(term)\n",
    "# Create final list of search terms, future month-year combinations deleted\n",
    "search_terms = search_terms[:-4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90939c22-d9d1-4d10-96fa-8d8581813121",
   "metadata": {},
   "source": [
    "We create a total of 124 search terms, for all month-year combinations between January 2012 and August 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e3ab8-6639-4f7e-93f9-9f391a2acce2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2a: Scraping Google search to retrieve list of links to DR articles\n",
    "With the following code we scrape Google searches to retrieve a list of links to DR articles, given the DR website does not provide a useful search function. We use Selenium to go execute a Google search and retrieve DR article links. We execute the search, save the HTML for the first results page and then go to further pages of the search results to retrieve more search resutls. Google intervenes when scaping search results too fast. We therefore integrate a break when moving between pages. The break time takes random values between 0.25 and 3.5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89324d-fc3a-484e-aa46-7cdc6e9f3ca5",
   "metadata": {},
   "source": [
    "Scraping the first three search pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e90353-f00c-4fcc-9974-03d9dcca753f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g1/m2905s6d6093hshyj_v9xm380000gn/T/ipykernel_2401/2112208343.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "100%|██████████| 1/1 [00:20<00:00, 20.56s/it]\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "html_list = []\n",
    "finished_searches = 0\n",
    "\n",
    "for i in tqdm.tqdm(search_terms[0:1]):\n",
    "    # Go to google\n",
    "    driver.get('https:google.com')\n",
    "    # Discard cookie message, reject cookies\n",
    "    cookie = driver.find_element(By.ID, \"W0wltc\")\n",
    "    cookie.click()\n",
    "    # Search for DR news articles\n",
    "    gsearch = driver.find_element(By.CSS_SELECTOR, \"input[title='Søg']\")\n",
    "    gsearch.send_keys(i)\n",
    "    # Get HTML for first search result page\n",
    "    html = driver.page_source\n",
    "    html_list.append(html)\n",
    "    # Go to next result page\n",
    "    next_page = driver.find_element(By.CSS_SELECTOR, \".NVbCr+ span\") #CSS selector only last not previous page\n",
    "    next_page.click()\n",
    "    # Define an error used when reaching last search page:\n",
    "        # When error = 0, there is another resut page.\n",
    "        # When error = 1, there is no further page on Google, loop stops.\n",
    "    error = 0\n",
    "    page = 1\n",
    "    for page in range(0,1): #Iterates over 3 Pages in total\n",
    "        try:\n",
    "            html2 = driver.page_source\n",
    "            html_list.append(html2)\n",
    "            # Google detects suspicious behavior and asks to solve some puzzle after 7 iterations. Trying random sleep time and scrolling down to element.\n",
    "            time.sleep(np.random.uniform(10, 15))\n",
    "            # Go to next result page\n",
    "            next_page = driver.find_element(By.CSS_SELECTOR, \"#pnnext .NVbCr+ span\") #CSS selector only last not previous page\n",
    "            next_page.click()\n",
    "            page += 1\n",
    "        except:\n",
    "            error += 1\n",
    "    finished_searches += 1\n",
    "    time.sleep(np.random.uniform(5,10))\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e5b25-4204-4c6c-aaf3-2e2756b781f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2b: Scraping number of google results for search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c18dc0d-9dff-4a1c-8560-c527fc69b995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]\n",
      "[WDM] - Downloading:   0%|                                                                 | 0.00/6.21M [00:00<?, ?B/s]\u001b[A\n",
      "[WDM] - Downloading:   7%|███▋                                                     | 416k/6.21M [00:00<00:01, 4.12MB/s]\u001b[A\n",
      "[WDM] - Downloading:  28%|███████████████▋                                        | 1.74M/6.21M [00:00<00:00, 9.75MB/s]\u001b[A\n",
      "[WDM] - Downloading: 100%|████████████████████████████████████████████████████████| 6.21M/6.21M [00:00<00:00, 17.1MB/s]\u001b[A\n",
      "C:\\Users\\jgb569\\AppData\\Local\\Temp/ipykernel_13496/2393136411.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:47<00:00, 11.86s/it]\n"
     ]
    }
   ],
   "source": [
    "num_search_results = []\n",
    "\n",
    "for search_term in tqdm.tqdm(search_terms[124:129]):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager())\n",
    "    # Go to google\n",
    "    driver.get('https:google.com')\n",
    "    # Discard cookie message, reject cookies\n",
    "    cookie = driver.find_element(By.ID, \"W0wltc\")\n",
    "    cookie.click()\n",
    "    # Search for DR news articles\n",
    "    gsearch = driver.find_element(By.CSS_SELECTOR, \"input[title='Søg']\")\n",
    "    gsearch.send_keys(search_term)\n",
    "    # Get HTML for first search result page\n",
    "    num_results = driver.find_element(By.ID, \"result-stats\")\n",
    "    num_search_results.append(num_results.text)\n",
    "    time.sleep(np.random.uniform(0.25,1))\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caac0f3-91b1-4ae5-80cd-5bbb818ddd10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3: Create link list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e53828fb-66d5-4a11-a00a-ed503001207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing two empty lists\n",
    "link_list = []\n",
    "link_list_clean = []\n",
    "# Iterating over the results from scraping\n",
    "for l in html_list:\n",
    "    soup = BeautifulSoup(l, 'lxml')\n",
    "    try: \n",
    "        links = soup.find('div', class_ = 'v7W49e').find_all('a', href=True)\n",
    "    except:\n",
    "        pass\n",
    "    # Generate list with all links\n",
    "    for i in links:\n",
    "        temp = i['href']\n",
    "        link_list.append(temp)\n",
    "    # Getting rid of noise, links not pointing to DR but Google infrastructure\n",
    "    for link in link_list:\n",
    "        if \"webcache.googleusercontent\" not in link:\n",
    "            link_list_clean.append(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5cdc6-5958-4f3c-b6a4-e081ca572c36",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 4: From list to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f3dd3ce-727a-4928-a22c-cd08cd59a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pd.DataFrame from link_list_clean\n",
    "dr_links = pd.DataFrame({'links':link_list_clean})\n",
    "# For some reason, there are many duplicates included, drop them\n",
    "dr_links = dr_links.drop_duplicates(subset = 'links')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814eb19-8e58-4a64-8e52-6f369c44d825",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 5: Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "647434dc-fd6a-43fc-b502-cb15f7870dde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dr_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdr_links\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdr_links.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dr_links' is not defined"
     ]
    }
   ],
   "source": [
    "dr_links.to_csv(\"dr_links.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c1be7-6e4f-405c-a238-9e10560682ef",
   "metadata": {},
   "source": [
    "### TV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df696bac-33e2-4cb9-941f-914d738d798b",
   "metadata": {},
   "source": [
    "#### 1:  Generate search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a577362-4312-49ef-84df-1141596390c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[]\n",
    "for year in range(2012, 2023, 1):\n",
    "    year=f\"https://search.tv2.dk/?query=sygeplejersker+{year}\"\n",
    "    \n",
    "    for month in range(1,10,1): #Måned 1-9\n",
    "        search_month=f\"-0{month}&sort=relevance&page=\"\n",
    "        \n",
    "        for page in range(1,11,1): # (start, stop, step)\n",
    "            url = year+search_month+f\"{page}\"\n",
    "            links.append(url)\n",
    "    \n",
    "    for month in range(10, 13): #måned 10-12\n",
    "        search_month=f\"-{month}&sort=relevance&page=\"\n",
    "        \n",
    "        for page in range(1,11,1): # (start, stop, step)\n",
    "            url = year+search_month+f\"{page}\"\n",
    "            links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d127176-ac6e-4ff7-a8b5-7b2bf0fa3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv2_article_list=pd.DataFrame(links)\n",
    "tv2_article_list.to_csv(\"tv2_links_to_articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1818fed-8182-469c-b9fa-97aea1e9c274",
   "metadata": {},
   "source": [
    "#### 2: Scraping TV2 searches for links to articles and create list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "229f1bdc-5b02-4540-971a-45957353d880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_url=[]\n",
    "for i in tqdm.tqdm(links[0:2]):\n",
    "    try:\n",
    "        resp_page = requests.get(i,headers={\"Name\" : \"Oliver Fredborg Smietana\" , \"email\": \"kph383@ku.alumni.dk\"})\n",
    "        soup = BeautifulSoup(resp_page.content, 'lxml')\n",
    "        for link in soup.find(\"section\").find_all('a', href=True):\n",
    "            article_url.append(links['href'])\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a188ee2-10cb-48fa-b69b-dc713b659076",
   "metadata": {},
   "source": [
    "#### 3: Scraping articles from links list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33f880f9-cab1-4b93-b565-bca66f5b8d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:19<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "titles_list=[]\n",
    "h2_list=[]\n",
    "date_list=[]\n",
    "content_list=[]\n",
    "author_list=[]\n",
    "sub_head_list=[]\n",
    "tag_list=[]\n",
    "\n",
    "#  Define start and end of article loop\n",
    "start=0\n",
    "end=15\n",
    "\n",
    "\n",
    "for u in tqdm.tqdm(article_url[start: end]) :\n",
    "    re=requests.get(u, headers={\"Name\" : \"Oliver Fredborg Smietana\" , \"email\": \"kph383@alumni.ku.dk\", \"Purpose\": \"exam project for Copenhagen uni Course\" })\n",
    "    soup=BeautifulSoup(re.content, \"lxml\")\n",
    "    try:\n",
    "        title=soup.find(\"h1\", class_=\"tc_heading tc_heading--2\")\n",
    "        titles_list.append(title.text.strip())\n",
    "    except:\n",
    "        titles_list.append(\"\")\n",
    "\n",
    "    try:\n",
    "        date=soup.find(\"time\", class_=\"tc_timestamp\")['datetime']\n",
    "        date_list.append(date)\n",
    "    except:\n",
    "        date_list.append(\"\")\n",
    "\n",
    "    try:\n",
    "        h2=soup.find_all(\"h2\", class_=\"tc_heading tc_heading--2\")\n",
    "        h2_i_list=[]\n",
    "        for i in h2:\n",
    "            h2_i=i.get_text()\n",
    "            h2_i_list.append(h2_i)\n",
    "        h2_str=\" \".join(h2_i_list)\n",
    "        h2_list.append(h2_str)\n",
    "    except:\n",
    "        h2_list.append(\"\")\n",
    "\n",
    "    try:\n",
    "        sub_head=soup.find(\"p\", class_=\"tc_page__body__standfirst\")\n",
    "        sub_head_list.append(sub_head.text.strip())\n",
    "    except:\n",
    "        sub_head_list.append(\"\")\n",
    "\n",
    "    try:\n",
    "        author=soup.find(\"span\", class_=\"tc_byline__author__text\")\n",
    "        author_list.append(author.text.strip())\n",
    "    except:\n",
    "         author_list.append(\"\")\n",
    "\n",
    "    try:\n",
    "        content_i_list=[]\n",
    "        content_i=soup.find(\"div\", class_=\"tc_richcontent\").find_all(\"p\")\n",
    "        for i in content_i:\n",
    "            content_i_list.append(i.text.strip())\n",
    "        content_str=\" \".join(content_i_list)\n",
    "        content_list.append(content_str)\n",
    "    except:\n",
    "        content_list.append(\"\")\n",
    "\n",
    "    try:\n",
    "        tag=soup.find(\"a\", class_=\"tc_label tc_label--color-base-red\")\n",
    "        tag_list.append(tag.text.strip())\n",
    "    except:\n",
    "        tag_list.append(\"\")\n",
    "\n",
    "    time.sleep(0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2aca4e-f6e8-4f33-a7c4-3321d9d144ba",
   "metadata": {},
   "source": [
    "#### 4: Creating dataframe from scraped articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c136aa59-efdf-445e-ba40-86cccd635b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv2_articles_sample=pd.DataFrame([titles_list, sub_head_list, h2_list, content_list, author_list, tag_list, date_list, links[start:end]]).transpose()\n",
    "tv2_articles_sample.columns=[\"titles\", \"sub_header\", \"h2\", \"content\", \"author\", \"tag\", \"date\", \"links\"]\n",
    "tv2_articles[\"source\"]=\"tv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f0c08a9-7c17-4a63-b6c4-348ae82ac59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv2_articles.to_csv(\"tv2_articles_sample.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ce8ba-b038-4567-8d02-7729b8b2c4ec",
   "metadata": {},
   "source": [
    "The complete data is loaded from the local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665ade2-3dc9-4519-b652-819e28dd55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv2_articles=pd.read_csv(\"tv2_articles.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
