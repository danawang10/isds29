{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24198316",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29eef3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import lemmy # For lemmatization\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "import itertools\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a25b4e",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7d76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sygeplej2x = pd.read_csv('ft_sygeplej2x.csv')\n",
    "dr_sygeplej2x = pd.read_csv('dr_sygeplej2x.csv')\n",
    "tv2_sygeplej2x = pd.read_csv('tv2_sygeplej2x.csv')\n",
    "\n",
    "ft_2 = ft_sygeplej2x.copy() \n",
    "dr_2 = dr_sygeplej2x.copy() \n",
    "tv2_2 = tv2_sygeplej2x.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363eb4eb",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da20202",
   "metadata": {},
   "source": [
    "## Remove non-alphanumerical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1106e258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgb569\\AppData\\Local\\Temp/ipykernel_17052/1145022020.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['content'] = df['content'].str.replace(r'\\W', ' ')\\\n"
     ]
    }
   ],
   "source": [
    "for df in [ft_2, dr_2, tv2_2]:\n",
    "    df['content'] = df['content'].str.replace(r'\\W', ' ')\\\n",
    "                                 .str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa89b8",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13384270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jgb569\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download tokenizer\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94dbe35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenizer function\n",
    "def tokenizer(df):\n",
    "    df_tokenslist = []\n",
    "    for document in tqdm.tqdm(df['content']):\n",
    "        tokens = nltk.tokenize.word_tokenize(document, language = 'danish')\n",
    "        df_tokenslist.append(tokens)\n",
    "    df_tokens = list(itertools.chain(*df_tokenslist))\n",
    "    return df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "464bbc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528/528 [00:01<00:00, 369.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3607/3607 [00:16<00:00, 219.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2943500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 296/296 [01:00<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9108918\n"
     ]
    }
   ],
   "source": [
    "dr_2_tokens = tokenizer(dr_2)\n",
    "print(len(dr_2_tokens))\n",
    "tv2_2_tokens = tokenizer(tv2_2)\n",
    "print(len(tv2_2_tokens))\n",
    "ft_2_tokens = tokenizer(ft_2)\n",
    "print(len(ft_2_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f029ee",
   "metadata": {},
   "source": [
    "## Remove stopwords and create word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b12ba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oliver/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Get stopwords list\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('danish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214d537",
   "metadata": {},
   "source": [
    "### Tokenized content for three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bbd8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from token-list\n",
    "dr_nostop = [word for word in dr_2_tokens if not word in stopwords]\n",
    "tv2_nostop = [word for word in tv2_2_tokens if not word in stopwords]\n",
    "ft_nostop = [word for word in ft_2_tokens if not word in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4dfc56",
   "metadata": {},
   "source": [
    "### Create set of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f661ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_complete = dr_nostop + tv2_nostop + ft_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0861d172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5356096"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39401dce",
   "metadata": {},
   "source": [
    "The total number of words across our three datasets is 5356096."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630d8e9",
   "metadata": {},
   "source": [
    "Our unique wordset contains 115189 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799d52d",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f7c87",
   "metadata": {},
   "source": [
    "### Stemming of entire wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa6495d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1247edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_stemmed_with_num = [stemmer.stem(word) for word in wordlist_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b64f854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75802"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete duplicates\n",
    "wordlist_stemmed_with_num = list(set(wordlist_stemmed_with_num))\n",
    "len(wordlist_stemmed_with_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a7e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "wordlist_stemmed = [word for word in wordlist_stemmed_with_num if not word.isdigit()]            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caeb02",
   "metadata": {},
   "source": [
    "### Stemming of content by source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17fa9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sourcestemmer(wordlist):\n",
    "    wordlist_stemmed = [stemmer.stem(word) for word in wordlist]\n",
    "    return wordlist_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "635ede8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_stemmed = sourcestemmer(dr_nostop)\n",
    "tv2_stemmed = sourcestemmer(tv2_nostop)\n",
    "ft_stemmed = sourcestemmer(ft_nostop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503e9cb",
   "metadata": {},
   "source": [
    "### Wordcount for complete content of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "415c2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting lists together to one large string as preprocessing for wordcount\n",
    "dr_string = \" \".join(dr_stemmed)\n",
    "tv2_string = \" \".join(tv2_stemmed)\n",
    "ft_string = \" \".join(ft_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "def8c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6972f1",
   "metadata": {},
   "source": [
    "#### DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b5a7730",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.96 GiB for an array with shape (125596, 9574) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17052/2318576253.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Make the bag to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Input the bag and the words into a dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmatrix_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmatrix_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.96 GiB for an array with shape (125596, 9574) and data type int64"
     ]
    }
   ],
   "source": [
    "#Store the class in 'count' to ease coding\n",
    "dr_bag = count.fit_transform(dr_stemmed) #fit_transform takes an array as input and outputs the bag of words\n",
    "\n",
    "dr_count_array = dr_bag.toarray() #Make the bag to an array\n",
    "dr_matrix = pd.DataFrame(data=dr_count_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "dr_matrix_sum = dr_matrix.sum().transpose()\n",
    "dr_matrix_sum.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37f0ceef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'list_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17052/2305459028.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'os' has no attribute 'list_dir'"
     ]
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6992eb5",
   "metadata": {},
   "source": [
    "#### TV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030140c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the class in 'count' to ease coding\n",
    "tv2_bag = count.fit_transform(tv2_stemmed) #fit_transform takes an array as input and outputs the bag of words\n",
    "\n",
    "tv2_array = tv2_bag.toarray() #Make the bag to an array\n",
    "tv2_matrix = pd.DataFrame(data=tv2_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "tv2_matrix_sum = tv2_matrix.sum().transpose()\n",
    "tv2_matrix_sum.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ecfab2",
   "metadata": {},
   "source": [
    "#### Folketinget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the class in 'count' to ease coding\n",
    "ft_bag = count.fit_transform(ft_stemmed) #fit_transform takes an array as input and outputs the bag of words\n",
    "\n",
    "ft_count_array = ft_bag.toarray() #Make the bag to an array\n",
    "ft_matrix = pd.DataFrame(data=ft_count_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "ft_matrix_sum = ft_matrix.sum().transpose()\n",
    "ft_matrix_sum.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d67a18",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bbe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Danish lemmatizer\n",
    "lem = lemmy.load(\"da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_lem = [lem.lemmatize(\"\", word) for word in wordset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list instead of list of list\n",
    "wordlist_lem = [word for sublist in wordlist_lem for word in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_lem_2 = wordlist_lem_2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab4464",
   "metadata": {},
   "source": [
    "Comment: The lemmatization returns a list of lists that also contains more than two words which could lead to problems.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a83f44",
   "metadata": {},
   "source": [
    "## Stemming and bag of words for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfeea19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
