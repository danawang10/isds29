{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f664d95e",
   "metadata": {},
   "source": [
    "# Google DR Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f16a2",
   "metadata": {},
   "source": [
    "## Questions\n",
    "* It seems that with Selenium, we do not provide the names. We only do when using the Request packages...:headers={\"Name\" : \"Simon Ullrich - summer course project\" , \"email\": \"simon.ullrich@sodas.ku.dk\"}\n",
    "\n",
    "## Comments\n",
    "* Looping until last page implemented\n",
    "    * Error term included to do while loop\n",
    "* Time.sleep included for less suspicious behaviour\n",
    "* CSS selector changed, because if last page is reached, the old CSS selecter might just have turned around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f2e9a",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5a9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import csv\n",
    "\n",
    "# Import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d6f55",
   "metadata": {},
   "source": [
    "## Generate search terms\n",
    "\n",
    "Search terms follow the following format: We search for the month and year in the format DR articles inclue a timestap. Manual Google searches proved to provide relevant research results mostly limited to the month provided. In this way we create a list of links to DR articles. All articles are located on the site https://www.dr.dk/nyheder or a subsite. This can be included in the google search. An example of a search is: jan. 2012\" AND \"sygeplejersker*\" site:https://www.dr.dk/nyheder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1e4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate empty list with search terms\n",
    "search_terms = []\n",
    "# Generate combinations of year-month and term search combinations\n",
    "for year in range(2012, 2023):\n",
    "    months = ['jan.', 'feb.', 'mar.', 'apr.', 'maj', 'jun.', 'jul.', 'aug.', 'sep.', 'okt.', 'nov.', 'dec.']\n",
    "    for month in months:\n",
    "        term = f'\"{month} {year}\" AND \"sygeplejersker*\" site:https://www.dr.dk/nyheder/ \\n'\n",
    "        search_terms.append(term)\n",
    "# Create final list of search terms, future month-year combinations deleted\n",
    "search_terms = search_terms[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9214b5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_terms[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64a096",
   "metadata": {},
   "source": [
    "We create a total of 128 search terms, for all month-year combinations between January 2012 and August 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda86a9f",
   "metadata": {},
   "source": [
    "## Scraping Google search to retrieve list of links to DR articles\n",
    "With the following code we scrape Google searches to retrieve a list of links to DR articles, given the DR website does not provide a useful search function. We use Selenium to go execute a Google search and retrieve DR article links. We execute the search, save the HTML for the first results page and then go to further pages of the search results to retrieve more search resutls. Google intervenes when scaping search results too fast. We therefore integrate a break when moving between pages. The break time takes random values between 0.25 and 3.5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4916ffc",
   "metadata": {},
   "source": [
    "### Defining a scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "edaf8e66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21476/469254787.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhtml_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfinished_searches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_terms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdriverService\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mService\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\jgb569\\OneDrive - University of Copenhagen\\Documents\\Webscraping\\chromedriver.exe\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriverService\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "html_list = []\n",
    "finished_searches = 0\n",
    "for i in tqdm.tqdm(search_terms[start:stop]):\n",
    "    driverService = Service(r\"C:\\Users\\jgb569\\OneDrive - University of Copenhagen\\Documents\\Webscraping\\chromedriver.exe\") \n",
    "    driver = webdriver.Chrome(service = driverService)\n",
    "    # Go to google\n",
    "    driver.get('https:google.com')\n",
    "    # Discard cookie message, reject cookies\n",
    "    cookie = driver.find_element(By.ID, \"W0wltc\")\n",
    "    cookie.click()\n",
    "    # Search for DR news articles\n",
    "    gsearch = driver.find_element(By.CSS_SELECTOR, \"input[title='Søg']\")\n",
    "    gsearch.send_keys(i)\n",
    "    # Get HTML for first search result page\n",
    "    html = driver.page_source\n",
    "    html_list.append(html)\n",
    "    # Go to next result page\n",
    "    next_page = driver.find_element(By.CSS_SELECTOR, \".NVbCr+ span\") #CSS selector only last not previous page\n",
    "    next_page.click()\n",
    "    # Define an error used when reaching last search page:\n",
    "        # When error = 0, there is another resut page.\n",
    "        # When error = 1, there is no further page on Google, loop stops.\n",
    "    error = 0 \n",
    "    while error < 1:\n",
    "        try:\n",
    "            html2 = driver.page_source\n",
    "            html_list.append(html2)\n",
    "            # Google detects suspicious behavior and asks to solve some puzzle after 7 iterations. Trying random sleep time and scrolling down to element.\n",
    "            time.sleep(np.random.uniform(5, 10))\n",
    "            # Go to next result page\n",
    "            next_page = driver.find_element(By.CSS_SELECTOR, \"#pnnext .NVbCr+ span\") #CSS selector only last not previous page\n",
    "            next_page.click()\n",
    "        except:\n",
    "            error += 1\n",
    "    finished_searches += 1\n",
    "    time.sleep(np.random.uniform(30-45))\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2fb75",
   "metadata": {},
   "source": [
    "### SCRAPE NOT HERE - PROBLEM: Page limit set, but continues to iterate over pages!!\n",
    "The call of the scraping function is divided into smaller portions for a less suspicious Selenium experience. This did not fully work.\n",
    "Jan-Aug 2012: All links\n",
    "Sep-Dec 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a794c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "html_list = []\n",
    "finished_searches = 8\n",
    "for i in tqdm.tqdm(search_terms[8:12]):\n",
    "    driverService = Service(r\"C:\\Users\\jgb569\\OneDrive - University of Copenhagen\\Documents\\Webscraping\\chromedriver.exe\") \n",
    "    driver = webdriver.Chrome(service = driverService)\n",
    "    # Go to google\n",
    "    driver.get('https:google.com')\n",
    "    # Discard cookie message, reject cookies\n",
    "    cookie = driver.find_element(By.ID, \"W0wltc\")\n",
    "    cookie.click()\n",
    "    # Search for DR news articles\n",
    "    gsearch = driver.find_element(By.CSS_SELECTOR, \"input[title='Søg']\")\n",
    "    gsearch.send_keys(i)\n",
    "    # Get HTML for first search result page\n",
    "    html = driver.page_source\n",
    "    html_list.append(html)\n",
    "    # Go to next result page\n",
    "    next_page = driver.find_element(By.CSS_SELECTOR, \".NVbCr+ span\") #CSS selector only last not previous page\n",
    "    next_page.click()\n",
    "    # Define an error used when reaching last search page:\n",
    "        # When error = 0, there is another resut page.\n",
    "        # When error = 1, there is no further page on Google, loop stops.\n",
    "    error = 0\n",
    "    page = 1\n",
    "    while page < 3:\n",
    "        try:\n",
    "            html2 = driver.page_source\n",
    "            html_list.append(html2)\n",
    "            # Google detects suspicious behavior and asks to solve some puzzle after 7 iterations. Trying random sleep time and scrolling down to element.\n",
    "            time.sleep(np.random.uniform(5, 12))\n",
    "            # Go to next result page\n",
    "            next_page = driver.find_element(By.CSS_SELECTOR, \"#pnnext .NVbCr+ span\") #CSS selector only last not previous page\n",
    "            next_page.click()\n",
    "            page += 1\n",
    "        except:\n",
    "            error += 1\n",
    "    finished_searches += 1\n",
    "    time.sleep(np.random.uniform(30,40))\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a3c8d",
   "metadata": {},
   "source": [
    "# Scrape HERE (First 3 pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c47e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [04:34<00:00, 21.12s/it]\n"
     ]
    }
   ],
   "source": [
    "html_list = []\n",
    "finished_searches = 111\n",
    "for i in tqdm.tqdm(search_terms[111:124]):\n",
    "    driverService = Service(r\"C:\\Users\\jgb569\\OneDrive - University of Copenhagen\\Documents\\Webscraping\\chromedriver.exe\") \n",
    "    driver = webdriver.Chrome(service = driverService)\n",
    "    # Go to google\n",
    "    driver.get('https:google.com')\n",
    "    # Discard cookie message, reject cookies\n",
    "    cookie = driver.find_element(By.ID, \"W0wltc\")\n",
    "    cookie.click()\n",
    "    # Search for DR news articles\n",
    "    gsearch = driver.find_element(By.CSS_SELECTOR, \"input[title='Søg']\")\n",
    "    gsearch.send_keys(i)\n",
    "    # Get HTML for first search result page\n",
    "    html = driver.page_source\n",
    "    html_list.append(html)\n",
    "    # Go to next result page\n",
    "    next_page = driver.find_element(By.CSS_SELECTOR, \".NVbCr+ span\") #CSS selector only last not previous page\n",
    "    next_page.click()\n",
    "    # Define an error used when reaching last search page:\n",
    "        # When error = 0, there is another resut page.\n",
    "        # When error = 1, there is no further page on Google, loop stops.\n",
    "    html2 = driver.page_source\n",
    "    html_list.append(html2)\n",
    "    # Google detects suspicious behavior and asks to solve some puzzle after 7 iterations. Trying random sleep time and scrolling down to element.\n",
    "    time.sleep(np.random.uniform(5, 12))\n",
    "    # Go to next result page\n",
    "    next_page = driver.find_element(By.CSS_SELECTOR, \"#pnnext .NVbCr+ span\") #CSS selector only last not previous page\n",
    "    next_page.click()\n",
    "    finished_searches += 1\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33955a",
   "metadata": {},
   "source": [
    "### Further search instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b6d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67767057",
   "metadata": {},
   "source": [
    "### Create link list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba9c7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing two empty lists\n",
    "link_list = []\n",
    "link_list_clean = []\n",
    "# Iterating over the results from scraping\n",
    "for l in html_list:\n",
    "    soup = BeautifulSoup(l, 'lxml')\n",
    "    try: \n",
    "        links = soup.find('div', class_ = 'v7W49e').find_all('a', href=True)\n",
    "    except:\n",
    "        pass\n",
    "    # Generate list with all links\n",
    "    for i in links:\n",
    "        temp = i['href']\n",
    "        link_list.append(temp)\n",
    "    # Getting rid of noise, links not pointing to DR but Google infrastructure\n",
    "    for link in link_list:\n",
    "        if \"webcache.googleusercontent\" not in link:\n",
    "            link_list_clean.append(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ba10b",
   "metadata": {},
   "source": [
    "#### From list to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecd2d50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4436\n"
     ]
    }
   ],
   "source": [
    "dr_links_012 = pd.DataFrame({'links':link_list_clean})\n",
    "print(len(dr_links_012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For the different scraping sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b8bb086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr_links_012_backup = dr_links_012.copy()\n",
    "dr_links_012_clean = dr_links_012_backup.drop_duplicates(subset = 'links')\n",
    "len(dr_links_012_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d65c43",
   "metadata": {},
   "source": [
    "* `dr_links_001` has 29525 instances. After dropping duplicates 713 links remain.\n",
    "* `dr_links_002` has 713 instances. After dropping duplicates 212 links remain.\n",
    "* `dr_links_003` has 14336 instances. After dropping duplicates 506 links remain.\n",
    "* `dr_links_004` has ?? instances. After dropping duplicates 80 links remain. (20 per month? Sep-Dec 2012, but I thought it is iterating over 3 pages)\n",
    "* `dr_links_005` has 2571 instances. After dropping duplicates 222 links remain. (20 per month? jan-nov 2013)\n",
    "* `dr_links_006` has 1323 instances. After dropping duplicates 155 links remain. \n",
    "* `dr_links_007` has 9307 instances. After dropping duplicates 447 links remain.\n",
    "* `dr_links_008` has 980 instances. After dropping duplicates 143 links remain.\n",
    "* `dr_links_009` has 16689 instances. After dropping duplicates 530 links remain.\n",
    "* `dr_links_010` has 17456 instances. After dropping duplicates 397 links remain.\n",
    "* `dr_links_011` has 313 instances. After dropping duplicates 64 links remain.\n",
    "* `dr_links_012` has 4436 instances. After dropping duplicates 244 links remain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccbc59f",
   "metadata": {},
   "source": [
    "#### Save first part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_001_clean.to_csv(\"dr_links_001_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "beb912f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_002_clean.to_csv(\"dr_links_002_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0f8f1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_003_clean.to_csv(\"dr_links_003_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4adac0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_004_clean.to_csv(\"dr_links_004_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cdd75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_005_clean.to_csv(\"dr_links_005_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44835741",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_006_clean.to_csv(\"dr_links_006_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff18ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_007_clean.to_csv(\"dr_links_007_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c08692c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_008_clean.to_csv(\"dr_links_008_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f696f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_009_clean.to_csv(\"dr_links_009_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c20f36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_010_clean.to_csv(\"dr_links_010_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc82678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_011_clean.to_csv(\"dr_links_011_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "664a8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_links_012_clean.to_csv(\"dr_links_012_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb9096",
   "metadata": {},
   "source": [
    "### Calling the scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5a4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ca285f9",
   "metadata": {},
   "source": [
    "# Backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a614cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated call to scraping function. But there is an issue with it. The pauses are not implemented, must be implemented in function itself\n",
    "\n",
    "html_list_dr = []\n",
    "finished_searches_list_dr = []\n",
    "for i in range(0, 32):\n",
    "    html_list, finished_searches = scrape_and_run(search_range[i], search_range[i+1])\n",
    "    html_list_dr.append(html_list)\n",
    "    finished_searches_list_dr.append(finished_searches)\n",
    "    time.sleep(np.random.uniform(10,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d943cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_list = []\n",
    "\n",
    "for i in tqdm.tqdm(search_terms):\n",
    "    driverService = Service(r\"C:\\Users\\jgb569\\OneDrive - University of Copenhagen\\Documents\\Webscraping\\chromedriver.exe\") \n",
    "    driver = webdriver.Chrome(service = driverService)\n",
    "    # Go to google\n",
    "    driver.get('https:google.com')\n",
    "    # Discard cookie message, reject cookies\n",
    "    cookie = driver.find_element(By.ID, \"W0wltc\")\n",
    "    cookie.click()\n",
    "    # Search for DR news articles\n",
    "    gsearch = driver.find_element(By.CSS_SELECTOR, \"input[title='Søg']\")\n",
    "    gsearch.send_keys(i)\n",
    "    # Get HTML for first search result page\n",
    "    html = driver.page_source\n",
    "    html_list.append(html)\n",
    "    # Go to next result page\n",
    "    next_page = driver.find_element(By.CSS_SELECTOR, \".NVbCr+ span\") #CSS selector only last not previous page\n",
    "    next_page.click()\n",
    "    # Define an error used when reaching last search page:\n",
    "        # When error = 0, there is another resut page.\n",
    "        # When error = 1, there is no further page on Google, loop stops.\n",
    "    error = 0 \n",
    "    while error < 1:\n",
    "        try:\n",
    "            html2 = driver.page_source\n",
    "            html_list.append(html2)\n",
    "            # Google detects suspicious behavior and asks to solve some puzzle after 7 iterations. Trying random sleep time and scrolling down to element.\n",
    "            time.sleep(np.random.uniform(0.25, 3.5))\n",
    "            # Go to next result page\n",
    "            next_page = driver.find_element(By.CSS_SELECTOR, \"#pnnext .NVbCr+ span\") #CSS selector only last not previous page\n",
    "            next_page.click()\n",
    "        except:\n",
    "            error += 1\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_run(start, stop):\n",
    "    '''\n",
    "    This function scrapes google results from the search_terms list.\n",
    "    It allows dividing the scraping into smaller parts, since the entire list does not run completely through.\n",
    "    \n",
    "    start: takes an index of the search_terms list\n",
    "    stop: takes a higher index of the search_terms list\n",
    "    '''\n",
    "    html_list = []\n",
    "    finished_searches = 0\n",
    "    for i in tqdm.tqdm(search_terms[start:stop]):\n",
    "        driverService = Service(r\"C:\\Users\\jgb569\\OneDrive - University of Copenhagen\\Documents\\Webscraping\\chromedriver.exe\") \n",
    "        driver = webdriver.Chrome(service = driverService)\n",
    "        # Go to google\n",
    "        driver.get('https:google.com')\n",
    "        # Discard cookie message, reject cookies\n",
    "        cookie = driver.find_element(By.ID, \"W0wltc\")\n",
    "        cookie.click()\n",
    "        # Search for DR news articles\n",
    "        gsearch = driver.find_element(By.CSS_SELECTOR, \"input[title='Søg']\")\n",
    "        gsearch.send_keys(i)\n",
    "        # Get HTML for first search result page\n",
    "        html = driver.page_source\n",
    "        html_list.append(html)\n",
    "        # Go to next result page\n",
    "        next_page = driver.find_element(By.CSS_SELECTOR, \".NVbCr+ span\") #CSS selector only last not previous page\n",
    "        next_page.click()\n",
    "        # Define an error used when reaching last search page:\n",
    "            # When error = 0, there is another resut page.\n",
    "            # When error = 1, there is no further page on Google, loop stops.\n",
    "        error = 0 \n",
    "        while error < 1:\n",
    "            try:\n",
    "                html2 = driver.page_source\n",
    "                html_list.append(html2)\n",
    "                # Google detects suspicious behavior and asks to solve some puzzle after 7 iterations. Trying random sleep time and scrolling down to element.\n",
    "                time.sleep(np.random.uniform(3, 5))\n",
    "                # Go to next result page\n",
    "                next_page = driver.find_element(By.CSS_SELECTOR, \"#pnnext .NVbCr+ span\") #CSS selector only last not previous page\n",
    "                next_page.click()\n",
    "            except:\n",
    "                error += 1\n",
    "        finished_searches += 1\n",
    "        driver.quit()\n",
    "    return html_list, finished_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714c4b8",
   "metadata": {},
   "source": [
    "## Same as For loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_list = []\n",
    "finished_searches = 0\n",
    "for i in tqdm.tqdm(search_terms[start:stop]):\n",
    "    driverService = Service(r\"C:\\Users\\jgb569\\OneDrive - University of Copenhagen\\Documents\\Webscraping\\chromedriver.exe\") \n",
    "    driver = webdriver.Chrome(service = driverService)\n",
    "    # Go to google\n",
    "    driver.get('https:google.com')\n",
    "    # Discard cookie message, reject cookies\n",
    "    cookie = driver.find_element(By.ID, \"W0wltc\")\n",
    "    cookie.click()\n",
    "    # Search for DR news articles\n",
    "    gsearch = driver.find_element(By.CSS_SELECTOR, \"input[title='Søg']\")\n",
    "    gsearch.send_keys(i)\n",
    "    # Get HTML for first search result page\n",
    "    html = driver.page_source\n",
    "    html_list.append(html)\n",
    "    # Go to next result page\n",
    "    next_page = driver.find_element(By.CSS_SELECTOR, \".NVbCr+ span\") #CSS selector only last not previous page\n",
    "    next_page.click()\n",
    "    # Define an error used when reaching last search page:\n",
    "        # When error = 0, there is another resut page.\n",
    "        # When error = 1, there is no further page on Google, loop stops.\n",
    "    error = 0 \n",
    "    while error < 1:\n",
    "        try:\n",
    "            html2 = driver.page_source\n",
    "            html_list.append(html2)\n",
    "            # Google detects suspicious behavior and asks to solve some puzzle after 7 iterations. Trying random sleep time and scrolling down to element.\n",
    "            time.sleep(np.random.uniform(5, 10))\n",
    "            # Go to next result page\n",
    "            next_page = driver.find_element(By.CSS_SELECTOR, \"#pnnext .NVbCr+ span\") #CSS selector only last not previous page\n",
    "            next_page.click()\n",
    "        except:\n",
    "            error += 1\n",
    "    finished_searches += 1\n",
    "    time.sleep(np.random.uniform(30-45))\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
